---
title: "How Uncertain Are Your Random Forest Predictions?"
author: "Sarah Tan"
date: '`r Sys.Date()`'
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to surfin: A R package to perform statistical inference for random forests}
  %\VignetteEngine{knitr::rmarkdown} 
  %\VignetteEncoding{UTF-8}
---

This vignette demonstrates how to use the surfin R package to compute uncertainty for random forest predictions. We will compare the U-statistics based variance estimate (Mentch & Hooker 2016) provided in this package with another estimate based on the infinitesimal jackknife (Wager, Hastie, Efron, 2014). The latter is provided in the R package randomForestCI by the authors of the paper. For a discussion of the differences between these two variance estimates, see Section ??? of Mentch et al. (Mentch & Hooker 2016).

In a foundational paper, Breiman (2001) showed that a random forest's sampling variance (by which we mean ....) is determined by two factors: the variance of each tree in the forest, and the correlation between trees. Intuitively, having trees that are certain about their predictions and less correlated with each other means the forest is reasonably certain .

One of the parameters driving correlation between individual trees is how many features they share. Hence, the higher the proportion of features sampled by each tree, the higher the probability that trees have features in common. However, this same parameter also affects the variance of an individual tree - the less features a tree is allowed to use, the more uncertain its prediction. Hence, 

## Setup

First, load packages needed for this example:
```{r}
library(surfin)
library(devtools)  # to install randomForestCI package from github
library(randomForest)  # to compare forest implementations
library(rpart) # for kyphosis data
```

Next, install and load the randomForestCI R package:
```{r}
install_github("swager/randomForestCI")
library(randomForestCI)
```

## Regression
We start with a regression example:
```{r}
x = cu.summary[,c("Price","Country","Reliability","Type")]
y = cu.summary$Mileage
keep = !is.na(y)
y = y[keep]
x = x[keep,]
keep = !apply(is.na(x),1,any)
y = y[keep]
x = x[keep,]
n = length(y)
train = sample(1:n,n*0.7)
test = setdiff(1:n,train)
xtrain = x[train,]
ytrain = y[train]
xtest = x[test,]
ytest = y[test]
```

To compute the U-statistics based variance estimate, we use sampling without replacement and specify B, the number of common observations, to be 10.
```{r}
fit = forest(xtrain,ytrain,xtest,ytest,var.type="ustat",B=10)
```

Check out what the forest object outputs:
```{r}
names(fit)
```

### Prediction 
There are a variety of prediction options to choose from:

1. On the train set using only out-of-bag train observations

2. On the train set using all train observations

3. On the test set using all test observations

```{r}
u_train_oob = fit$predicted        # Case (1)
u_train_all = predict(fit,xtrain)  # Case (2)
u_test_all = fit$test$predicted    # Case (3)
temp = data.frame(u_train_oob,u_train_all)
head(temp)
head(u_test_all)
```

### U-statistics based variance estimate

Now we calculate and plot the u-statistics based variance estimate on the train set (Case (1)):
```{r}
ustat = forest.varU(fit$predictedAll,fit)
head(ustat)
plot(ustat)
```
We see that the ...  

We can also calculate the u-statistics based variance estimate on the test set (Case (2)):
```{r}
ustat = forest.varU(fit$test$predictedAll,fit)
head(ustat)
plot(ustat)
```

### Infinitesimal jackknife based variance estimate

Now we use the randomForestCI package to compute the infinitesimal jackknife based variance estimate (example taken from https://github.com/swager/randomForestCI):
```{r}
rf = randomForest(xtrain, ytrain, keep.inbag = TRUE) 
ij = randomForestInfJack(rf, xtrain, calibrate = TRUE)
head(ij)
plot(ij)
```

Next we try our own implementation of the infinitesimal jackknife (we don't have some extras implemented by the authors of the randomForestCI package, such as their calibration, but this is nice as a comparison). To compute the infinitesimal jackknife variance, we need sampling with replacement. 
```{r}
fit = forest(xtrain,ytrain,var.type="infjack")
ij2_train_oob = fit$predicted   # Case (1)
ij2 = forest.varIJ(fit$predictedAll,fit)
head(ij2)
plot(ij2)
```

### Compare to randomForest package

Let's compare our forest's predictions to the predictions of the randomForest package:
```{r}
rf_train_oob = rf$predicted
plot(ij2_train_oob,rf_train_oob)
lines(ij2_train_oob,ij2_train_oob,lty="dashed")
```

## Binary Classification

Next, we try classification. We currently only support binary classification:
```{r}
x = kyphosis[,c("Age","Number","Start")]
y = kyphosis$Kyphosis
n = length(y)
train = sample(1:n,n*0.7)
test = setdiff(1:n,train)
xtrain = x[train,]
ytrain = y[train]
xtest = x[test,]
ytest = y[test]
```

### Prediction

Like for regression, we have several options for prediction:
```{r}
fit = forest(xtrain,ytrain,xtest,ytest,var.type="ustat",B=10)
names(fit)
u_train_oob = fit$predicted        # Case (1)
u_train_all = predict(fit,xtrain)  # Case (2)
u_test_all = fit$test$predicted    # Case (3)
temp = data.frame(u_train_oob,u_train_all)
head(temp)
head(u_test_all)
```

### U-statistics based variance estimate

Calculate and plot the u-statistics based variance estimate on the train set predicted responses (Case (1)):
```{r}
ustat = forest.varU(fit$predictedAll,fit)
head(ustat)
plot(ustat)
```

We can also estimate the variance of the train set predicted probabilities. Note: there is a bug here because probability > 1, need to look closer at how it's being output.
```{r}
ustat = forest.varU(fit$predictedProb,fit)
head(ustat)
plot(ustat)
```

Now try the same but on the test set (Case (2)):
```{r}
ustat = forest.varU(fit$test$predictedAll,fit)
head(ustat)
plot(ustat)
ustat = forest.varU(fit$test$predictedProb,fit)
head(ustat)
plot(ustat)
```

### Infinitesimal jackknife based variance estimate

Next we compute the infinitesimal jackknife variance: (looks like there is a bug in the randomForestCI code in converting labels for classification to numeric, so the code below is commented out until that package is fixed)
```{r}
#rf = randomForest(x, y, keep.inbag = TRUE)
#ij = randomForestInfJack(rf, x, calibrate = TRUE)
#head(ij)
#plot(ij)
```

Our own implementation of the infinitesimal jackknife variance:
```{r}
fit = forest(xtrain,ytrain,var.type="infjack")
ij2_train_oob = fit$predicted   # Case (1)
ij2 = forest.varIJ(fit$predictedAll,fit)
head(ij2)
plot(ij2)
```

### Compare to randomForest package

We end by comparing our forest's predictions to that of the randomForest package
```{r}
rf = randomForest(xtrain,ytrain,keep.forest=TRUE,keep.inbag=TRUE,replace=TRUE)
rf_train_oob = rf$predicted
table(ij2_train_oob,rf_train_oob)
```

## Implementation Notes 
- Data with non-response, or classification with more than 2 categories is not yet supported. Contact us if you are eager for these!
- Categorical predictors are currently converted to their numeric equivalents, not made into indicator variables. This feature is pending. 
- Like the randomForest package, the splitting criterion for regression is mean squared error, and gini impurity for binary classification.